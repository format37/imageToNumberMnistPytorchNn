{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1401\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0931\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0675\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1176\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1276\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0567\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0667\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0236\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0165\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0405\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0622\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0326\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0246\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0093\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0365\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0114\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0478\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0021\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0327\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0066\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0063\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0283\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0109\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0510\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0136\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0112\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0133\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0057\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0288\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0285\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.94 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawSymbol(testArray,arrayRange):\n",
    "    dataiter = iter(testArray)\n",
    "    myimages, labels = dataiter.next()\n",
    "    myimages = myimages.numpy()\n",
    "    # Peeking into dataset\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for image in np.arange(arrayRange):\n",
    "        ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(myimages[image]), cmap='gray')\n",
    "        ax.set_title(str(labels[image].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHgAAACECAYAAABbNO+wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABIpJREFUeJztnE0opVEYx5/DWKi7QTQWXDIbG/kYpUGRjViynZkVG9koSylLS1P2M2VjarKQZKKYCQsWikSYBklDLFDCdGY1xz1vc7nXeN/33v/5/0o9T+dwH/16zrnvp9JaC8ElI+wCiL9QMDgUDA4Fg0PB4FAwOBQMjhOClVKXnp/fSqkPYdcVBC/CLiAItNaRv7FSKiIixyLyObyKgsOJDvbQISK/RORb2IUEgYuC34vIJ+3IOVrlyP8pIiJKqaiI7InIK631j7DrCQLXOvitiHx3Ra6Ie4LficjHsIsIEmeWaKXUGxH5KiIvtdYXYdcTFC518HsR+eKSXBGHOthVXOpgJ6FgcCgYHAoGJ6mLDUopfiNLHU611vmPTWIHpy8/E5lEweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAxO6G/Z6ezstPKuri4THx0dWWPX19cmHhsbs8aOj49NvLOz85wlpjXsYHAoGJykHgD349mkvb09Ky8pKXnS37m4uH9wf2Nj439KehKHh4cmHh4etsZWVlb8+MhVrfXrxyaxg8GhYHAoGJzQD5NiD4tERCoqKky8ublpjZWXl5u4urraGmtqajJxXV2dNXZwcGDioqKihGu7u7uz8pOTExMXFhbG/b39/X0r92kPTgh2MDgUDE7oS/Ts7OyDeSzT09Nxx3JyckxcWVlpja2urpq4trY24dpiz5yJiGxvb5vYu33k5uaaeHd3N+HP8Bt2MDgUDA4FgxP6qcp0oqOjw8Tj4+PW2Pr6uombm5utsbOzMz/K4alKQsHwhH6YlMoUFBRY+ejoqIkzMuzeGBoaMrFPS/KTYAeDQ8HgUDA43IMfoKenx8rz8+9f7np+fm6NbW1tBVJTsrCDwaFgcHgmy0N9fb2J5+bmrLGsrCwTx95gICKysLDga13/gGeyCAXDQ8Hg8DDJQ1tbm4lj91wR+26TpaWlwGr6H9jB4FAwOBQMjvN7cHZ2tpW3traa+ObmxhobHBw08e3trb+FPRPsYHAoGBznl+j+/n4rr6qqMrH3RvvFxcVAanpO2MHgUDA4FAyOc5cL29vbrXxiYsLKr66uTBx7yCQisry87F9hycPLhYSC4XHiMCkvL8/EIyMj1lhmZqaVT01NmTjFluQnwQ4Gh4LBoWBwIPdg774ae8qxtLTUGvO+T2NgYMC/wkKAHQwOBYMDuUSXlZVZeU1NTdy5fX19Vp5Kr0B6DtjB4FAwOBQMDsweHI1GTTwzMxN3nvcOjsnJSd9qSgXYweBQMDgwS3R3d7eJi4uL486bn5+38mRueEhH2MHgUDA4FAxO2u7BDQ0NVt7b2xtSJakNOxgcCgYnbZfoxsZGK49EInHnxl4hury89K2mVIQdDA4Fg0PB4KTtHvwQa2trVt7S0mLiVHobexCwg8GhYHCce3wUCD4+SigYHgoGJ9nDpFMR+elHISRpoo9PSfJLFkk/uESDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg/MHXjsUaGGEjSYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawSymbol(test_loader,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    data_path = 'data/test/'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHgAAACECAYAAABbNO+wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABAdJREFUeJztnD9IHEEUh9+LFhfRQjCdkk4bS0vthMM/qAiCRbAWPRQUMYiIRUBBsVW7gGCjRCwU7AWbHIhiIdiIkErR7ggIkyJh2DkQ7zazt7u//X3VG96yM3cfb/ZmdvfUGCMElw9xD4BECwWDQ8HgUDA4FAwOBYNDweBkRrCqFlT1p6r+VtXvcY+nVtTHPYAa8ktEvolIXkQ+xjyWmpEZwcaYHyIiqtolIq0xD6dmZGaKzioUDA4Fg0PB4GTmR5aq1svfz1snInWqmhORV2PMa7wji5YsVfCyiJRE5KuIfPkXL8c6ohqgvOGPTZYqOJNQMDgUDA4Fg1PVMklV+YssOTwaYz69dxArOL3cV3IQBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDE5mHnx/i+7ubqf9/Pxs45ubm1oPxzusYHAoGJzMT9GFQsFpj4+PxzSSaGAFg0PB4FAwOJm7Bu/s7DjtKK655Uuv8/Nz731UCisYHAoGJxNT9OTkpI1vb28j6WNwcNDGBwcHTm57e9vGc3NzkfT/FqxgcCgYHAoGp6o/YUnL+8Gtre5fUT48PNhYVb30sbq66rSfnp5s3NfX5+T6+/u99FlG0RjT9d5BrGBwKBgcyGXSycmJ0x4dHfVy3o2NDRu3tbU5ueDya2Zmxkt/PmAFg0PB4FAwODDX4LOzMxtfXl46uaOjo1DnHBoactrT09M2Pjw8dHJ7e3s2vru7C9VfFLCCwaFgcFK7kzUxMeG0t7a2bNzS0hL6vJ2dnTa+vr52ci8vLzZubm4O3YcnuJNFKBgeCgYnVdfg9vZ2G19dXTm5XC7npY/g93F6eurkBgYGvPThCV6DCQXDk6qdrGKxaOORkZFQ55ifn3fam5ubTntpacnGa2trofpIEqxgcCgYHAoGJ9HX4IuLC6fd2Nho4/X1dSdXKpVs3Nvb6+RmZ2dt3NTU5OSmpqacdvAhdQRYweBQMDgUDE7itioXFxdtvLCw4OSCtwFXVlac3NjYmI2Dtw5FRBoaGmxc/j5wT09P+MHGC7cqCQXDk7gpenh42MbHx8dezhn8jL7eTUoAnKIJBcNDweAkbqvSx3W3/HdFipdC/w0rGBwKBidxU3RYgu8m7e7uOrk4/2kubljB4FAwOBQMTuK2Kisln8877eA7wMG7R8Bwq5JQMDypXSZ1dHQ47f39/ZhGkmxYweBQMDgUDE5ql0mEyyQiFAwPBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FT74PujiNxHMRBSNZ8rOaiqpypJ+uAUDQ4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAzOH/uj3EORTEv7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawSymbol(load_dataset(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#external dataset testing\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in load_dataset():\n",
    "        #print(len(images),len(labels))\n",
    "        images = images.to(device)        \n",
    "        labels = labels.to(device)\n",
    "        #print(type(images))\n",
    "        #print(images.shape)\n",
    "        outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5749, -6.5254, -3.2973, -6.1618,  2.7998, -5.4755, -6.7113, -3.8188,\n",
       "         -1.0687, -5.5204]], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
